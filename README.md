# RUN INSTRUCTIONS (for linux)

Important - don't run as su.

1.  Replace the .env USERNAME variable with your username.

2.  Create `/secrets´folder with the files: db_password.txt and wp_admin_password.txt.

3.  first time - `sudo make` then go to `https://<your username>.42.fr`

stop containers - `sudo make stop`

delete containers (also stops) - `sudo make down`

restart containers (re-creates if necessary) - `sudo make up`

delete containers and volumes - `sudo make clean`

full clean (also removes /data/) - `sudo make fclean`

# DOCKER

## [Install Docker Desktop](https://docs.docker.com/desktop/install/linux/ubuntu/)

## Dockerfile

Docker builds images by reading the instructions from a Dockerfile. A Dockerfile is a text file containing instructions for building your source code. The Dockerfile instruction syntax is defined by the specification reference in the Dockerfile reference.

`FROM <image>` - Defines a base for your image.

`RUN <command>` - Executes any commands in a new **layer** on top of the current image and commits the result. RUN also has a shell for running commands.

`WORKDIR <directory>` - Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it in the Dockerfile.

`COPY <src> <dest>` - Copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.

`CMD <command>` - Lets you define the default program that is run once you start the container based on this image. Each Dockerfile only has one CMD, and only the last CMD instance is respected when multiple exist.

### Docker images

Docker images consist of layers. Each layer is the result of a build instruction in the Dockerfile. Layers are stacked sequentially, and each one is a delta representing the changes applied to the previous layer.



### Basic commands (from https://docker-curriculum.com)

`docker build -t [NAME] .` - create image with name NAME using the Dockerfile in the current directory

`docker images` - list images

`docker run [NAME]` - create container from image

`docker ps` - list running containers

`docker ps -a` - list all containers

`docker container prune` - delete all containers that have a status of exited

`docker run [container] --rm` - automatically deletes container after stopping

`docker run -d -p 8080:80 --name [container] IMAGE` - creates container with terminal detached, forwarding port 80 of the container to port 8080 on the  host, and with the name [container]

`docker port [container]` - list [container] ports

`docker stop [container]` - stops [container]

execute running container - `docker exec -it [container-id] /bin/sh`

force restart docker - `sudo systemctl restart docker.socket docker.service`

`docker compose docker-compose.yml up` - start container(s).

`docker compose docker-compose.yml stop` - stop containers.

`docker compose docker-compose.yml down` - stop and remove containers.

`docker compose config` - show compose configuration

## Docker-compose

Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file.

### The Compose Application Model

1.  Computing components of an application are defined as **services**. A service is an abstract concept implemented on platforms by running the same container image, and configuration, one or more times.

2.  Services communicate with each other through **networks**. In the Compose Specification, a network is a platform capability abstraction to establish an IP route between containers within services connected together.

3.  Services store and share persistent data into **volumes**. The Specification describes such a persistent data as a high-level filesystem mount with global options.

4.  A **secret** is a specific flavor of configuration data for sensitive data that should not be exposed without security considerations. Secrets are made available to services as files mounted into their containers, but the platform-specific resources to provide sensitive data are specific enough to deserve a distinct concept and definition within the Compose specification.

[Services top-level elements](https://docs.docker.com/reference/compose-file/services/)

## Volumes

[Docker compose volumes](https://kinsta.com/blog/docker-compose-volumes/)

[Volumes](https://docs.docker.com/storage/volumes/) vs [Bind mounts](https://docs.docker.com/storage/bind-mounts/)

[Volumes in Docker Compose](https://docs.docker.com/compose/compose-file/07-volumes/)

Docker volumes are a crucial ecosystem component that stores and manages persistent data generated by ephemeral containers. They enable data to persist even after removing or updating a container so that essential application data isn’t lost during routine operations.

A key advantage of using volumes over bind mounts, which are directory mounts from the host system to a container, is portability. You can quickly move volumes between different hosts or containers, but you must tie bind mounts to a specific directory on the host system.

This portability enables more flexible and efficient data management in container-based applications. Volumes are also compatible with various storage drivers, allowing you to choose the best storage solution for your specific use case.

While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:

-   Volumes are easier to back up or migrate than bind mounts.
You can manage volumes using Docker CLI commands or the Docker API.
-   Volumes work on both Linux and Windows containers.
-   Volumes can be more safely shared among multiple containers.
-   New volumes can have their content pre-populated by a container.
-   Volumes on Docker Desktop have much higher performance than bind mounts from Mac and Windows hosts.

[Understanding Docker Volumes](https://medium.com/@moein.moeinnia/understanding-docker-mounts-volumes-bind-mounts-and-tmpfs-f992185edc27)

As of today, there are three types of mounts in Docker, each designed for specific reasons and use cases. These types of mounts are:
-   **Bind Mounts** - multiple containers can access the same host folder. Rely on a host folder structure, it's not managed by docker. Map host (or remote) folders to a container folder.
-   **Volume mounts** - Volume mounts are like bind mounts but they are fully managed by docker itself. Volumes are stored in the Linux VM rather than the host. Volume mounts are a way to manage and store data separately from the containers themselves. They provide a means to store and share files and directories that can be persist even if containers are deleted or recreated.
-   **Tmpfs mounts** - special type of mount that allows you to create a temporary file system in memory. tmpfs mount is temporary and exists only as long as the container is running. Once the container is stopped or restarted, the data in the tmpfs mount is lost. Therefore, tmpfs mounts are not suitable for storing data that needs to persist across container restarts.

`docker volume ls` - list volumes

`docker volume inspect <volume_name>` - inspect specific volume

`docker volume rm <volume_name>` - remove volume

`docker volume prune` - remove unused volumes

## Networking

[Networking Overview](https://docs.docker.com/engine/network/)

You can create custom, user-defined networks, and connect multiple containers to the same network. Once connected to a user-defined network, containers can communicate with each other using container IP addresses or container names.

**If you want to make a container accessible to other containers, it isn't necessary to publish the container's ports. You can enable inter-container communication by connecting the containers to the same network, usually a bridge network.**

### Drivers

-   bridge - the default network driver.
-   host - Remove network isolation between the container and the Docker host.
-   none - Completely isolate a container from the host and other containers.
-   overlay - Overlay networks connect multiple Docker daemons together.
-   ipvlan - IPvlan networks provide full control over both IPv4 and IPv6 addressing.
-   macvlan - Assign a MAC address to a container.

### [Bridge network driver](https://docs.docker.com/engine/network/drivers/bridge/)

In terms of Docker, a bridge network uses a software bridge which lets containers connected to the same bridge network communicate, while providing isolation from containers that aren't connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks can't communicate directly with each other.

User-defined bridge networks are superior to the default bridge network.

Containers connected to the same user-defined bridge network effectively expose all ports to each other. For a port to be accessible to containers or non-Docker hosts on different networks, that port must be published using the -p or --publish flag.

The default bridge network is considered a legacy detail of Docker and is not recommended for production use.  you do not specify a network using the --network flag, and you do specify a network driver, your container is connected to the default bridge network by default. Containers connected to the default bridge network can communicate, but only by IP address.

`docker network create <network_name>`- create network

`docker network remove <network_name>` - remove network

`docker network connect <network_name> <container_name>` - connect a running container to an existing user-defined bridge.



## Secrets

[How to use Secrets](https://docs.docker.com/compose/how-tos/use-secrets/)

A secret is any piece of data, such as a password, certificate, or API key, that shouldn’t be transmitted over a network or stored unencrypted in a Dockerfile or in your application’s source code.

Services can only access secrets when explicitly granted by a secrets attribute within the services top-level element.

The secrets attribute grants access to sensitive data defined by the secrets top-level element on a per-service basis. Services can be granted access to multiple secrets.

Getting a secret into a container is a two-step process. First, define the secret using the top-level secrets element in your Compose file. Next, update your service definitions to reference the secrets they require with the secrets attribute. Compose grants access to secrets on a per-service basis.


# NGINX

[Reference guide](https://spinupwp.com/hosting-wordpress-yourself-nginx-php-mysql/)

[Beginner's Guide](https://nginx.org/en/docs/beginners_guide.html)

## Dockerfile

1.  Install debian:oldstable
2.  Update and upgrade
3.  Install nginx and openssl (to create the tls certificate)
4.  Create key and certificate
5.  Run nginx


## SSL TLS

[intro](https://www.youtube.com/watch?v=EnY6fSng3Ew)

[Install TLS](https://www.digitalocean.com/community/tutorials/how-to-create-a-self-signed-ssl-certificate-for-nginx-in-ubuntu-20-04-1)

[OpenSSL essentials](https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs)

**Self-signed certificate** - A self-signed certificate is a certificate that is signed with its own private key. Self-signed certificates can be used to encrypt data just as well as CA-signed certificates, but your users will be displayed a warning that says that the certificate is not trusted by their computer or browser. Therefore, self-signed certificates should only be used if you do not need to prove your service’s identity to its users (e.g. non-production or non-public servers).

### Create certificate

**openssl** - cryptography toolkit implementing the Secure Sockets Layer (SSL v2/v3) and Transport Layer Security (TLS v1) network protocols and related cryptography standards required by them.

The **-x509 option** tells req to create a self-signed certificate instead of generating a certificate signing request. The **-days 365** option specifies that the certificate will be valid for 365 days. The **-nodes option** specifies that the private key should not be encrypted with a pass phrase. 

[Settings](https://cipherlist.eu/)

[testssh.sh](https://github.com/drwetter/testssl.sh)

### Configuring nginx to use SSL

1.  Change nginx.conf to listen on port 443 instead of 80, and using ssl.
2.  Define ssl_certificate and ssl_certificate_key.

### FastCGI

nginx can be used to route requests to FastCGI servers which run applications built with various frameworks and programming languages such as PHP.

The most basic nginx configuration to work with a FastCGI server includes using the fastcgi_pass directive instead of the proxy_pass directive, and fastcgi_param directives to set parameters passed to a FastCGI server.

**location /**: In this location block, a try_files directive is used to check for files that match individual URI requests. Instead of returning a 404 Not Found status as a default, however, you’ll pass control to WordPress’s index.php file with the request arguments.

**location ~ \.php$**: This location block will handle PHP processing and proxy these requests to your wordpress container. Because your WordPress Docker image will be based on the php:fpm image, you will also include configuration options that are specific to the FastCGI protocol in this block. Nginx requires an independent PHP processor for PHP requests. In this case, these requests will be handled by the php-fpm processor that’s included with the php:fpm image. Additionally, this location block includes FastCGI-specific directives, variables, and options that will proxy requests to the WordPress application running in your wordpress container, set the preferred index for the parsed request URI, and parse URI requests.

# MARIADB

## Installing and configuring

[install](https://www.digitalocean.com/community/tutorials/how-to-install-mariadb-on-ubuntu-20-04)

[mariadb configuration](https://mariadb.com/kb/en/configuring-mariadb-for-remote-client-access/)

[mariadb-secure-installation](mariadb-secure-installation)

50-server.cnf defines **init_file** = /etc/mysql/init.sql and **root** as the user, and comments out **bind-address** = 127.0.0.1, thus allowing remote connections.

## Dockerfile

1.  Install debian:oldstable
2.  Update and upgrade
3.  Install mariadb-server
4.  copy conf file
5.  Create init.sql file, with db name, db user and db password.
6.  Start mariadbd service in background and run mariadb-secure-installtion (recommended)
7.  Stop mariadbd
8.  Start mariadbd in front

## Basic mysql commands


mariadb -u [user] -p [password]

[SQL Basic commands](https://www.w3schools.com/mysql/mysql_select.asp)

CREATE DATABASE IF NOT EXISTS [db_name];

<!-- Show current user -->
SELECT USER();

<!-- Show all users, with hosts and passwords -->
SELECT user,host,password FROM mysql.user;

<!-- Create database -->
CREATE DATABASE [database_name];

<!-- Show databases -->
SHOW DATABASES;

<!-- Select database -->
USE [database];

<!-- Use database -->
USE [database];

<!-- Show tables -->
SHOW TABLES;

<!-- Describe table -->
DESCRIBE [table];

<!-- Show all data from a table -->
SELECT * FROM [table];

# WORDPRESS

[wp-cli](https://wp-cli.org/)

## Dockerfile

1.  Install php and dependencies, install mariadb-client
2.  Wait for the mariadb container to be healthy
3.  Check if wp-config exists, if not:
4.  Install wp-cli and download wp core, create wp-config file and install wp core.
5.  Run php-fpm7.4 -F (foreground)